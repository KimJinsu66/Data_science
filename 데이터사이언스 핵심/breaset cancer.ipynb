{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load breaset cancer data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "#print(cancer.feature_names)\n",
    "#print(cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer to DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer['feature_names'], [\"target\"]))\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: data\n",
    "x = data.drop([\"target\"], axis=1)\n",
    "#x.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y: class\n",
    "y = data[\"target\"].astype(int)\n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "x = sc.fit_transform(x) # x_train data scaling\n",
    "#print(x)\n",
    "#print(x_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data 와 test deta를 나눈다. 468개의 training data 와 100개의 test data\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "# eval_size = 0.10\n",
    "kf = KFold(n_splits = 6) # KFold 객체 생성\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    #print(\"TRAIN:\",train_index,\"Test:\", test_index)\n",
    "    x_train , x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67358128\n",
      "Iteration 2, loss = 0.66689756\n",
      "Iteration 3, loss = 0.65971417\n",
      "Iteration 4, loss = 0.65347850\n",
      "Iteration 5, loss = 0.63323185\n",
      "Iteration 6, loss = 0.60793620\n",
      "Iteration 7, loss = 0.58093997\n",
      "Iteration 8, loss = 0.52885594\n",
      "Iteration 9, loss = 0.46403986\n",
      "Iteration 10, loss = 0.38211128\n",
      "Iteration 11, loss = 0.30123818\n",
      "Iteration 12, loss = 0.23381805\n",
      "Iteration 13, loss = 0.18807274\n",
      "Iteration 14, loss = 0.15871766\n",
      "Iteration 15, loss = 0.13785769\n",
      "Iteration 16, loss = 0.12095931\n",
      "Iteration 17, loss = 0.10755936\n",
      "Iteration 18, loss = 0.09747654\n",
      "Iteration 19, loss = 0.09033283\n",
      "Iteration 20, loss = 0.08401009\n",
      "Iteration 21, loss = 0.08070789\n",
      "Iteration 22, loss = 0.07821352\n",
      "Iteration 23, loss = 0.07525975\n",
      "Iteration 24, loss = 0.07446106\n",
      "Iteration 25, loss = 0.07284487\n",
      "Iteration 26, loss = 0.07251250\n",
      "Iteration 27, loss = 0.07056606\n",
      "Iteration 28, loss = 0.06930481\n",
      "Iteration 29, loss = 0.06895528\n",
      "Iteration 30, loss = 0.06764324\n",
      "Iteration 31, loss = 0.06716707\n",
      "Iteration 32, loss = 0.06692641\n",
      "Iteration 33, loss = 0.06712496\n",
      "Iteration 34, loss = 0.06547159\n",
      "Iteration 35, loss = 0.06613930\n",
      "Iteration 36, loss = 0.06486993\n",
      "Iteration 37, loss = 0.06499208\n",
      "Iteration 38, loss = 0.06350266\n",
      "Iteration 39, loss = 0.06515568\n",
      "Iteration 40, loss = 0.06400290\n",
      "Iteration 41, loss = 0.06405350\n",
      "Iteration 42, loss = 0.06216719\n",
      "Iteration 43, loss = 0.06244868\n",
      "Iteration 44, loss = 0.06151677\n",
      "Iteration 45, loss = 0.06117409\n",
      "Iteration 46, loss = 0.06088307\n",
      "Iteration 47, loss = 0.06074558\n",
      "Iteration 48, loss = 0.06122688\n",
      "Iteration 49, loss = 0.06103666\n",
      "Iteration 50, loss = 0.06083094\n",
      "Iteration 51, loss = 0.06003212\n",
      "Iteration 52, loss = 0.05914617\n",
      "Iteration 53, loss = 0.06046056\n",
      "Iteration 54, loss = 0.05949011\n",
      "Iteration 55, loss = 0.05932007\n",
      "Iteration 56, loss = 0.05840821\n",
      "Iteration 57, loss = 0.05837328\n",
      "Iteration 58, loss = 0.05788736\n",
      "Iteration 59, loss = 0.05816788\n",
      "Iteration 60, loss = 0.05776773\n",
      "Iteration 61, loss = 0.05749392\n",
      "Iteration 62, loss = 0.05749148\n",
      "Iteration 63, loss = 0.05748957\n",
      "Iteration 64, loss = 0.05683861\n",
      "Iteration 65, loss = 0.05672614\n",
      "Iteration 66, loss = 0.05684229\n",
      "Iteration 67, loss = 0.05709262\n",
      "Iteration 68, loss = 0.05665689\n",
      "Iteration 69, loss = 0.05590338\n",
      "Iteration 70, loss = 0.05695270\n",
      "Iteration 71, loss = 0.05629455\n",
      "Iteration 72, loss = 0.05564009\n",
      "Iteration 73, loss = 0.05539848\n",
      "Iteration 74, loss = 0.05549913\n",
      "Iteration 75, loss = 0.05513088\n",
      "Iteration 76, loss = 0.05619597\n",
      "Iteration 77, loss = 0.05566657\n",
      "Iteration 78, loss = 0.05595933\n",
      "Iteration 79, loss = 0.05474914\n",
      "Iteration 80, loss = 0.05522653\n",
      "Iteration 81, loss = 0.05531585\n",
      "Iteration 82, loss = 0.05551439\n",
      "Iteration 83, loss = 0.05511064\n",
      "Iteration 84, loss = 0.05462156\n",
      "Iteration 85, loss = 0.05405889\n",
      "Iteration 86, loss = 0.05394758\n",
      "Iteration 87, loss = 0.05363633\n",
      "Iteration 88, loss = 0.05368468\n",
      "Iteration 89, loss = 0.05351494\n",
      "Iteration 90, loss = 0.05347233\n",
      "Iteration 91, loss = 0.05353528\n",
      "Iteration 92, loss = 0.05321282\n",
      "Iteration 93, loss = 0.05330593\n",
      "Iteration 94, loss = 0.05358176\n",
      "Iteration 95, loss = 0.05334831\n",
      "Iteration 96, loss = 0.05324059\n",
      "Iteration 97, loss = 0.05362943\n",
      "Iteration 98, loss = 0.05245167\n",
      "Iteration 99, loss = 0.05270234\n",
      "Iteration 100, loss = 0.05256461\n",
      "Iteration 101, loss = 0.05243694\n",
      "Iteration 102, loss = 0.05204936\n",
      "Iteration 103, loss = 0.05297567\n",
      "Iteration 104, loss = 0.05271799\n",
      "Iteration 105, loss = 0.05325770\n",
      "Iteration 106, loss = 0.05284294\n",
      "Iteration 107, loss = 0.05147742\n",
      "Iteration 108, loss = 0.05386840\n",
      "Iteration 109, loss = 0.05190239\n",
      "Iteration 110, loss = 0.05171246\n",
      "Iteration 111, loss = 0.05193901\n",
      "Iteration 112, loss = 0.05126946\n",
      "Iteration 113, loss = 0.05136403\n",
      "Iteration 114, loss = 0.05133603\n",
      "Iteration 115, loss = 0.05154899\n",
      "Iteration 116, loss = 0.05072497\n",
      "Iteration 117, loss = 0.05218532\n",
      "Iteration 118, loss = 0.05131318\n",
      "Iteration 119, loss = 0.05085291\n",
      "Iteration 120, loss = 0.05117661\n",
      "Iteration 121, loss = 0.05094352\n",
      "Iteration 122, loss = 0.05107155\n",
      "Iteration 123, loss = 0.05076282\n",
      "Iteration 124, loss = 0.05097012\n",
      "Iteration 125, loss = 0.05088392\n",
      "Iteration 126, loss = 0.05124183\n",
      "Iteration 127, loss = 0.05470168\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# ANN(Training)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 0, shuffle =True)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(150,100),# 은닉층 수 ex) (300,100) 유닛 10개짜리 은닉층 2개\n",
    "               activation = 'logistic', #  적용함수 = 활성 함수(로지스틱 커브)\n",
    "               #activation = 'identity', # 학습속도가 엄청 빠름 결과 값 비교해보기\n",
    "               #activation = 'tanh', #결과값 제일 좋다\n",
    "               #activation = 'relu', # 결과값 best\n",
    "               solver = 'sgd', # algorithm (sgd말고 2개 더있음 Gradient descent이해 하면 금방 이해한다.)\n",
    "               tol = 1e-7,  # 우리가 러닝을 했는대 더이상 낮아지지 않으면 공부 그만해 라는 뜻\n",
    "               learning_rate_init = .1,\n",
    "               verbose = True) # 보고 할 것인가 안할 것인가. False로 바꾸면 lossfuction값 안나옴\n",
    "\n",
    "\n",
    "model1 = mlp.fit(x_train,y_train) # model 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50445764\n",
      "Iteration 2, loss = 0.08873442\n",
      "Iteration 3, loss = 0.08850713\n",
      "Iteration 4, loss = 0.08314871\n",
      "Iteration 5, loss = 0.08029820\n",
      "Iteration 6, loss = 0.08673859\n",
      "Iteration 7, loss = 0.07162173\n",
      "Iteration 8, loss = 0.07184191\n",
      "Iteration 9, loss = 0.06214207\n",
      "Iteration 10, loss = 0.05979554\n",
      "Iteration 11, loss = 0.06095995\n",
      "Iteration 12, loss = 0.06558168\n",
      "Iteration 13, loss = 0.05484179\n",
      "Iteration 14, loss = 0.05709055\n",
      "Iteration 15, loss = 0.05455465\n",
      "Iteration 16, loss = 0.05559524\n",
      "Iteration 17, loss = 0.05414385\n",
      "Iteration 18, loss = 0.05217978\n",
      "Iteration 19, loss = 0.05136229\n",
      "Iteration 20, loss = 0.05197306\n",
      "Iteration 21, loss = 0.05249195\n",
      "Iteration 22, loss = 0.04977459\n",
      "Iteration 23, loss = 0.06099737\n",
      "Iteration 24, loss = 0.05348768\n",
      "Iteration 25, loss = 0.05522908\n",
      "Iteration 26, loss = 0.05600768\n",
      "Iteration 27, loss = 0.06205692\n",
      "Iteration 28, loss = 0.05637996\n",
      "Iteration 29, loss = 0.05076163\n",
      "Iteration 30, loss = 0.05203233\n",
      "Iteration 31, loss = 0.04918791\n",
      "Iteration 32, loss = 0.05067926\n",
      "Iteration 33, loss = 0.04894733\n",
      "Iteration 34, loss = 0.04900627\n",
      "Iteration 35, loss = 0.04677403\n",
      "Iteration 36, loss = 0.04866778\n",
      "Iteration 37, loss = 0.05246757\n",
      "Iteration 38, loss = 0.04545899\n",
      "Iteration 39, loss = 0.04633008\n",
      "Iteration 40, loss = 0.04571968\n",
      "Iteration 41, loss = 0.04619769\n",
      "Iteration 42, loss = 0.04870461\n",
      "Iteration 43, loss = 0.04795052\n",
      "Iteration 44, loss = 0.04519718\n",
      "Iteration 45, loss = 0.04656828\n",
      "Iteration 46, loss = 0.04642863\n",
      "Iteration 47, loss = 0.04851919\n",
      "Iteration 48, loss = 0.04901953\n",
      "Iteration 49, loss = 0.05452085\n",
      "Iteration 50, loss = 0.04797224\n",
      "Iteration 51, loss = 0.04746644\n",
      "Iteration 52, loss = 0.04688636\n",
      "Iteration 53, loss = 0.04571541\n",
      "Iteration 54, loss = 0.04301528\n",
      "Iteration 55, loss = 0.04867558\n",
      "Iteration 56, loss = 0.04606803\n",
      "Iteration 57, loss = 0.04430929\n",
      "Iteration 58, loss = 0.04416324\n",
      "Iteration 59, loss = 0.04425498\n",
      "Iteration 60, loss = 0.04294335\n",
      "Iteration 61, loss = 0.04299961\n",
      "Iteration 62, loss = 0.04313349\n",
      "Iteration 63, loss = 0.04367901\n",
      "Iteration 64, loss = 0.04538168\n",
      "Iteration 65, loss = 0.06148944\n",
      "Iteration 66, loss = 0.04644471\n",
      "Iteration 67, loss = 0.04654528\n",
      "Iteration 68, loss = 0.06000774\n",
      "Iteration 69, loss = 0.04255733\n",
      "Iteration 70, loss = 0.04334345\n",
      "Iteration 71, loss = 0.04430146\n",
      "Iteration 72, loss = 0.04354240\n",
      "Iteration 73, loss = 0.04322768\n",
      "Iteration 74, loss = 0.04204629\n",
      "Iteration 75, loss = 0.04189868\n",
      "Iteration 76, loss = 0.04362829\n",
      "Iteration 77, loss = 0.04165562\n",
      "Iteration 78, loss = 0.04308371\n",
      "Iteration 79, loss = 0.04189558\n",
      "Iteration 80, loss = 0.04090602\n",
      "Iteration 81, loss = 0.04374960\n",
      "Iteration 82, loss = 0.04330928\n",
      "Iteration 83, loss = 0.04163520\n",
      "Iteration 84, loss = 0.04138556\n",
      "Iteration 85, loss = 0.04357932\n",
      "Iteration 86, loss = 0.04542194\n",
      "Iteration 87, loss = 0.04582117\n",
      "Iteration 88, loss = 0.04048940\n",
      "Iteration 89, loss = 0.04469456\n",
      "Iteration 90, loss = 0.05407649\n",
      "Iteration 91, loss = 0.04725202\n",
      "Iteration 92, loss = 0.04913953\n",
      "Iteration 93, loss = 0.04372661\n",
      "Iteration 94, loss = 0.04289204\n",
      "Iteration 95, loss = 0.04286911\n",
      "Iteration 96, loss = 0.04298035\n",
      "Iteration 97, loss = 0.04170732\n",
      "Iteration 98, loss = 0.04225283\n",
      "Iteration 99, loss = 0.04247825\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# ANN(Training)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 0, shuffle =True)\n",
    "\n",
    "\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(100,100),# 은닉층 수 ex) (300,100) 유닛 10개짜리 은닉층 2개\n",
    "               #activation = 'logistic', #  적용함수 = 활성 함수(로지스틱 커브)\n",
    "               activation = 'identity', # 학습속도가 엄청 빠름 결과 값 비교해보기\n",
    "               #activation = 'tanh', #결과값 제일 좋다\n",
    "               #activation = 'relu', # 결과값 best\n",
    "               solver = 'sgd', # algorithm (sgd말고 2개 더있음 Gradient descent이해 하면 금방 이해한다.)\n",
    "               tol = 1e-7,  # 우리가 러닝을 했는대 더이상 낮아지지 않으면 공부 그만해 라는 뜻\n",
    "               learning_rate_init = .1,\n",
    "               verbose = True) # 보고 할 것인가 안할 것인가. False로 바꾸면 lossfuction값 안나옴\n",
    "\n",
    "\n",
    "model1 = mlp1.fit(x_train,y_train) # model 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38959288\n",
      "Iteration 2, loss = 0.09150667\n",
      "Iteration 3, loss = 0.07351924\n",
      "Iteration 4, loss = 0.06884792\n",
      "Iteration 5, loss = 0.06643081\n",
      "Iteration 6, loss = 0.06235078\n",
      "Iteration 7, loss = 0.06049419\n",
      "Iteration 8, loss = 0.05852376\n",
      "Iteration 9, loss = 0.05736686\n",
      "Iteration 10, loss = 0.05407331\n",
      "Iteration 11, loss = 0.05058953\n",
      "Iteration 12, loss = 0.04834716\n",
      "Iteration 13, loss = 0.04773870\n",
      "Iteration 14, loss = 0.04553178\n",
      "Iteration 15, loss = 0.04435460\n",
      "Iteration 16, loss = 0.04338662\n",
      "Iteration 17, loss = 0.04195909\n",
      "Iteration 18, loss = 0.04213259\n",
      "Iteration 19, loss = 0.04046173\n",
      "Iteration 20, loss = 0.03935997\n",
      "Iteration 21, loss = 0.03933348\n",
      "Iteration 22, loss = 0.03825684\n",
      "Iteration 23, loss = 0.03686698\n",
      "Iteration 24, loss = 0.03733002\n",
      "Iteration 25, loss = 0.03560175\n",
      "Iteration 26, loss = 0.03473873\n",
      "Iteration 27, loss = 0.03556280\n",
      "Iteration 28, loss = 0.03331607\n",
      "Iteration 29, loss = 0.03162688\n",
      "Iteration 30, loss = 0.03138260\n",
      "Iteration 31, loss = 0.03063745\n",
      "Iteration 32, loss = 0.02990449\n",
      "Iteration 33, loss = 0.02901845\n",
      "Iteration 34, loss = 0.02772634\n",
      "Iteration 35, loss = 0.02742127\n",
      "Iteration 36, loss = 0.02606835\n",
      "Iteration 37, loss = 0.02550652\n",
      "Iteration 38, loss = 0.02477463\n",
      "Iteration 39, loss = 0.02433441\n",
      "Iteration 40, loss = 0.02421473\n",
      "Iteration 41, loss = 0.02265050\n",
      "Iteration 42, loss = 0.02166947\n",
      "Iteration 43, loss = 0.02126766\n",
      "Iteration 44, loss = 0.02017487\n",
      "Iteration 45, loss = 0.02094055\n",
      "Iteration 46, loss = 0.02132051\n",
      "Iteration 47, loss = 0.01889454\n",
      "Iteration 48, loss = 0.01769626\n",
      "Iteration 49, loss = 0.01801104\n",
      "Iteration 50, loss = 0.01660537\n",
      "Iteration 51, loss = 0.01586257\n",
      "Iteration 52, loss = 0.01532626\n",
      "Iteration 53, loss = 0.01543783\n",
      "Iteration 54, loss = 0.01466052\n",
      "Iteration 55, loss = 0.01430233\n",
      "Iteration 56, loss = 0.01497738\n",
      "Iteration 57, loss = 0.01364596\n",
      "Iteration 58, loss = 0.01216674\n",
      "Iteration 59, loss = 0.01247927\n",
      "Iteration 60, loss = 0.01146800\n",
      "Iteration 61, loss = 0.01205160\n",
      "Iteration 62, loss = 0.01281420\n",
      "Iteration 63, loss = 0.01173534\n",
      "Iteration 64, loss = 0.01060190\n",
      "Iteration 65, loss = 0.00968258\n",
      "Iteration 66, loss = 0.00960385\n",
      "Iteration 67, loss = 0.00993258\n",
      "Iteration 68, loss = 0.00914600\n",
      "Iteration 69, loss = 0.00848643\n",
      "Iteration 70, loss = 0.00848616\n",
      "Iteration 71, loss = 0.00806174\n",
      "Iteration 72, loss = 0.00797128\n",
      "Iteration 73, loss = 0.00758309\n",
      "Iteration 74, loss = 0.00743772\n",
      "Iteration 75, loss = 0.00697405\n",
      "Iteration 76, loss = 0.00691926\n",
      "Iteration 77, loss = 0.00668891\n",
      "Iteration 78, loss = 0.00673173\n",
      "Iteration 79, loss = 0.00622936\n",
      "Iteration 80, loss = 0.00604601\n",
      "Iteration 81, loss = 0.00583588\n",
      "Iteration 82, loss = 0.00580544\n",
      "Iteration 83, loss = 0.00586464\n",
      "Iteration 84, loss = 0.00526818\n",
      "Iteration 85, loss = 0.00545027\n",
      "Iteration 86, loss = 0.00528537\n",
      "Iteration 87, loss = 0.00520034\n",
      "Iteration 88, loss = 0.00490045\n",
      "Iteration 89, loss = 0.00472812\n",
      "Iteration 90, loss = 0.00472050\n",
      "Iteration 91, loss = 0.00442593\n",
      "Iteration 92, loss = 0.00433549\n",
      "Iteration 93, loss = 0.00418147\n",
      "Iteration 94, loss = 0.00415095\n",
      "Iteration 95, loss = 0.00410159\n",
      "Iteration 96, loss = 0.00386010\n",
      "Iteration 97, loss = 0.00379529\n",
      "Iteration 98, loss = 0.00378146\n",
      "Iteration 99, loss = 0.00381328\n",
      "Iteration 100, loss = 0.00356251\n",
      "Iteration 101, loss = 0.00346108\n",
      "Iteration 102, loss = 0.00329157\n",
      "Iteration 103, loss = 0.00336919\n",
      "Iteration 104, loss = 0.00320141\n",
      "Iteration 105, loss = 0.00315661\n",
      "Iteration 106, loss = 0.00305391\n",
      "Iteration 107, loss = 0.00304473\n",
      "Iteration 108, loss = 0.00295623\n",
      "Iteration 109, loss = 0.00286837\n",
      "Iteration 110, loss = 0.00280450\n",
      "Iteration 111, loss = 0.00277799\n",
      "Iteration 112, loss = 0.00270919\n",
      "Iteration 113, loss = 0.00265339\n",
      "Iteration 114, loss = 0.00261729\n",
      "Iteration 115, loss = 0.00263328\n",
      "Iteration 116, loss = 0.00254244\n",
      "Iteration 117, loss = 0.00246592\n",
      "Iteration 118, loss = 0.00239835\n",
      "Iteration 119, loss = 0.00240248\n",
      "Iteration 120, loss = 0.00237149\n",
      "Iteration 121, loss = 0.00232894\n",
      "Iteration 122, loss = 0.00225650\n",
      "Iteration 123, loss = 0.00221966\n",
      "Iteration 124, loss = 0.00223577\n",
      "Iteration 125, loss = 0.00221724\n",
      "Iteration 126, loss = 0.00213155\n",
      "Iteration 127, loss = 0.00207420\n",
      "Iteration 128, loss = 0.00203979\n",
      "Iteration 129, loss = 0.00203498\n",
      "Iteration 130, loss = 0.00200332\n",
      "Iteration 131, loss = 0.00198512\n",
      "Iteration 132, loss = 0.00191326\n",
      "Iteration 133, loss = 0.00185360\n",
      "Iteration 134, loss = 0.00184897\n",
      "Iteration 135, loss = 0.00185459\n",
      "Iteration 136, loss = 0.00182180\n",
      "Iteration 137, loss = 0.00179095\n",
      "Iteration 138, loss = 0.00172279\n",
      "Iteration 139, loss = 0.00171157\n",
      "Iteration 140, loss = 0.00169463\n",
      "Iteration 141, loss = 0.00169052\n",
      "Iteration 142, loss = 0.00169382\n",
      "Iteration 143, loss = 0.00165263\n",
      "Iteration 144, loss = 0.00162100\n",
      "Iteration 145, loss = 0.00157295\n",
      "Iteration 146, loss = 0.00156052\n",
      "Iteration 147, loss = 0.00154950\n",
      "Iteration 148, loss = 0.00152542\n",
      "Iteration 149, loss = 0.00150185\n",
      "Iteration 150, loss = 0.00146484\n",
      "Iteration 151, loss = 0.00143799\n",
      "Iteration 152, loss = 0.00145251\n",
      "Iteration 153, loss = 0.00144151\n",
      "Iteration 154, loss = 0.00143958\n",
      "Iteration 155, loss = 0.00141171\n",
      "Iteration 156, loss = 0.00135749\n",
      "Iteration 157, loss = 0.00131678\n",
      "Iteration 158, loss = 0.00132394\n",
      "Iteration 159, loss = 0.00131943\n",
      "Iteration 160, loss = 0.00129310\n",
      "Iteration 161, loss = 0.00126566\n",
      "Iteration 162, loss = 0.00125517\n",
      "Iteration 163, loss = 0.00124472\n",
      "Iteration 164, loss = 0.00123326\n",
      "Iteration 165, loss = 0.00122061\n",
      "Iteration 166, loss = 0.00121728\n",
      "Iteration 167, loss = 0.00118251\n",
      "Iteration 168, loss = 0.00116532\n",
      "Iteration 169, loss = 0.00120532\n",
      "Iteration 170, loss = 0.00116353\n",
      "Iteration 171, loss = 0.00115143\n",
      "Iteration 172, loss = 0.00111621\n",
      "Iteration 173, loss = 0.00111111\n",
      "Iteration 174, loss = 0.00110698\n",
      "Iteration 175, loss = 0.00109274\n",
      "Iteration 176, loss = 0.00107941\n",
      "Iteration 177, loss = 0.00105838\n",
      "Iteration 178, loss = 0.00107994\n",
      "Iteration 179, loss = 0.00105688\n",
      "Iteration 180, loss = 0.00103650\n",
      "Iteration 181, loss = 0.00103512\n",
      "Iteration 182, loss = 0.00108102\n",
      "Iteration 183, loss = 0.00106105\n",
      "Iteration 184, loss = 0.00101311\n",
      "Iteration 185, loss = 0.00098150\n",
      "Iteration 186, loss = 0.00098238\n",
      "Iteration 187, loss = 0.00100783\n",
      "Iteration 188, loss = 0.00095772\n",
      "Iteration 189, loss = 0.00097038\n",
      "Iteration 190, loss = 0.00096389\n",
      "Iteration 191, loss = 0.00093923\n",
      "Iteration 192, loss = 0.00092827\n",
      "Iteration 193, loss = 0.00091246\n",
      "Iteration 194, loss = 0.00090877\n",
      "Iteration 195, loss = 0.00089431\n",
      "Iteration 196, loss = 0.00088443\n",
      "Iteration 197, loss = 0.00087816\n",
      "Iteration 198, loss = 0.00087053\n",
      "Iteration 199, loss = 0.00087259\n",
      "Iteration 200, loss = 0.00085828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# ANN(Training)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 0, shuffle =True)\n",
    "\n",
    "\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(150,100),# 은닉층 수 ex) (300,100) 유닛 10개짜리 은닉층 2개\n",
    "               #activation = 'logistic', #  적용함수 = 활성 함수(로지스틱 커브)\n",
    "               #activation = 'identity', # 학습속도가 엄청 빠름 결과 값 비교해보기\n",
    "               activation = 'tanh', #결과값 제일 좋다\n",
    "               #activation = 'relu', # 결과값 best\n",
    "               solver = 'sgd', # algorithm (sgd말고 2개 더있음 Gradient descent이해 하면 금방 이해한다.)\n",
    "               tol = 1e-7,  # 우리가 러닝을 했는대 더이상 낮아지지 않으면 공부 그만해 라는 뜻\n",
    "               learning_rate_init = .1,\n",
    "               verbose = True) # 보고 할 것인가 안할 것인가. False로 바꾸면 lossfuction값 안나옴\n",
    "\n",
    "\n",
    "model2 = mlp2.fit(x_train,y_train) # model 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54516604\n",
      "Iteration 2, loss = 0.19281710\n",
      "Iteration 3, loss = 0.12290005\n",
      "Iteration 4, loss = 0.08982186\n",
      "Iteration 5, loss = 0.07098029\n",
      "Iteration 6, loss = 0.06332192\n",
      "Iteration 7, loss = 0.06189580\n",
      "Iteration 8, loss = 0.05699473\n",
      "Iteration 9, loss = 0.04984898\n",
      "Iteration 10, loss = 0.04445891\n",
      "Iteration 11, loss = 0.04125148\n",
      "Iteration 12, loss = 0.03825579\n",
      "Iteration 13, loss = 0.03507386\n",
      "Iteration 14, loss = 0.03317500\n",
      "Iteration 15, loss = 0.03122510\n",
      "Iteration 16, loss = 0.02918763\n",
      "Iteration 17, loss = 0.02709998\n",
      "Iteration 18, loss = 0.02562066\n",
      "Iteration 19, loss = 0.02385393\n",
      "Iteration 20, loss = 0.02239486\n",
      "Iteration 21, loss = 0.02133055\n",
      "Iteration 22, loss = 0.01981591\n",
      "Iteration 23, loss = 0.01862711\n",
      "Iteration 24, loss = 0.01760350\n",
      "Iteration 25, loss = 0.01673803\n",
      "Iteration 26, loss = 0.01564174\n",
      "Iteration 27, loss = 0.01468805\n",
      "Iteration 28, loss = 0.01398213\n",
      "Iteration 29, loss = 0.01310862\n",
      "Iteration 30, loss = 0.01257622\n",
      "Iteration 31, loss = 0.01205920\n",
      "Iteration 32, loss = 0.01135103\n",
      "Iteration 33, loss = 0.01064080\n",
      "Iteration 34, loss = 0.01013423\n",
      "Iteration 35, loss = 0.00951528\n",
      "Iteration 36, loss = 0.00902662\n",
      "Iteration 37, loss = 0.00887664\n",
      "Iteration 38, loss = 0.00825164\n",
      "Iteration 39, loss = 0.00779551\n",
      "Iteration 40, loss = 0.00752270\n",
      "Iteration 41, loss = 0.00711028\n",
      "Iteration 42, loss = 0.00686276\n",
      "Iteration 43, loss = 0.00648989\n",
      "Iteration 44, loss = 0.00628407\n",
      "Iteration 45, loss = 0.00593766\n",
      "Iteration 46, loss = 0.00564459\n",
      "Iteration 47, loss = 0.00543252\n",
      "Iteration 48, loss = 0.00531972\n",
      "Iteration 49, loss = 0.00502912\n",
      "Iteration 50, loss = 0.00494016\n",
      "Iteration 51, loss = 0.00463013\n",
      "Iteration 52, loss = 0.00450707\n",
      "Iteration 53, loss = 0.00432106\n",
      "Iteration 54, loss = 0.00419106\n",
      "Iteration 55, loss = 0.00414048\n",
      "Iteration 56, loss = 0.00387989\n",
      "Iteration 57, loss = 0.00373470\n",
      "Iteration 58, loss = 0.00361964\n",
      "Iteration 59, loss = 0.00349793\n",
      "Iteration 60, loss = 0.00341863\n",
      "Iteration 61, loss = 0.00327160\n",
      "Iteration 62, loss = 0.00319628\n",
      "Iteration 63, loss = 0.00309014\n",
      "Iteration 64, loss = 0.00298513\n",
      "Iteration 65, loss = 0.00290021\n",
      "Iteration 66, loss = 0.00286583\n",
      "Iteration 67, loss = 0.00275165\n",
      "Iteration 68, loss = 0.00268294\n",
      "Iteration 69, loss = 0.00257913\n",
      "Iteration 70, loss = 0.00249581\n",
      "Iteration 71, loss = 0.00246463\n",
      "Iteration 72, loss = 0.00235875\n",
      "Iteration 73, loss = 0.00233272\n",
      "Iteration 74, loss = 0.00229090\n",
      "Iteration 75, loss = 0.00223373\n",
      "Iteration 76, loss = 0.00217740\n",
      "Iteration 77, loss = 0.00211292\n",
      "Iteration 78, loss = 0.00205375\n",
      "Iteration 79, loss = 0.00202867\n",
      "Iteration 80, loss = 0.00196254\n",
      "Iteration 81, loss = 0.00190867\n",
      "Iteration 82, loss = 0.00185348\n",
      "Iteration 83, loss = 0.00184454\n",
      "Iteration 84, loss = 0.00178094\n",
      "Iteration 85, loss = 0.00178522\n",
      "Iteration 86, loss = 0.00172684\n",
      "Iteration 87, loss = 0.00169640\n",
      "Iteration 88, loss = 0.00167334\n",
      "Iteration 89, loss = 0.00161909\n",
      "Iteration 90, loss = 0.00160434\n",
      "Iteration 91, loss = 0.00156747\n",
      "Iteration 92, loss = 0.00154633\n",
      "Iteration 93, loss = 0.00152299\n",
      "Iteration 94, loss = 0.00148066\n",
      "Iteration 95, loss = 0.00146421\n",
      "Iteration 96, loss = 0.00144914\n",
      "Iteration 97, loss = 0.00142091\n",
      "Iteration 98, loss = 0.00139140\n",
      "Iteration 99, loss = 0.00136743\n",
      "Iteration 100, loss = 0.00134974\n",
      "Iteration 101, loss = 0.00133420\n",
      "Iteration 102, loss = 0.00131048\n",
      "Iteration 103, loss = 0.00128059\n",
      "Iteration 104, loss = 0.00126591\n",
      "Iteration 105, loss = 0.00124631\n",
      "Iteration 106, loss = 0.00122673\n",
      "Iteration 107, loss = 0.00121364\n",
      "Iteration 108, loss = 0.00119399\n",
      "Iteration 109, loss = 0.00117626\n",
      "Iteration 110, loss = 0.00115860\n",
      "Iteration 111, loss = 0.00114417\n",
      "Iteration 112, loss = 0.00113392\n",
      "Iteration 113, loss = 0.00111603\n",
      "Iteration 114, loss = 0.00111010\n",
      "Iteration 115, loss = 0.00109729\n",
      "Iteration 116, loss = 0.00108148\n",
      "Iteration 117, loss = 0.00105464\n",
      "Iteration 118, loss = 0.00104797\n",
      "Iteration 119, loss = 0.00103923\n",
      "Iteration 120, loss = 0.00101914\n",
      "Iteration 121, loss = 0.00100176\n",
      "Iteration 122, loss = 0.00098498\n",
      "Iteration 123, loss = 0.00097475\n",
      "Iteration 124, loss = 0.00096613\n",
      "Iteration 125, loss = 0.00095187\n",
      "Iteration 126, loss = 0.00093381\n",
      "Iteration 127, loss = 0.00093421\n",
      "Iteration 128, loss = 0.00092424\n",
      "Iteration 129, loss = 0.00090308\n",
      "Iteration 130, loss = 0.00088972\n",
      "Iteration 131, loss = 0.00089029\n",
      "Iteration 132, loss = 0.00086378\n",
      "Iteration 133, loss = 0.00087028\n",
      "Iteration 134, loss = 0.00086451\n",
      "Iteration 135, loss = 0.00085110\n",
      "Iteration 136, loss = 0.00083811\n",
      "Iteration 137, loss = 0.00081887\n",
      "Iteration 138, loss = 0.00080987\n",
      "Iteration 139, loss = 0.00081025\n",
      "Iteration 140, loss = 0.00079232\n",
      "Iteration 141, loss = 0.00078333\n",
      "Iteration 142, loss = 0.00077383\n",
      "Iteration 143, loss = 0.00076732\n",
      "Iteration 144, loss = 0.00076177\n",
      "Iteration 145, loss = 0.00075871\n",
      "Iteration 146, loss = 0.00074519\n",
      "Iteration 147, loss = 0.00073656\n",
      "Iteration 148, loss = 0.00072844\n",
      "Iteration 149, loss = 0.00072150\n",
      "Iteration 150, loss = 0.00071827\n",
      "Iteration 151, loss = 0.00070890\n",
      "Iteration 152, loss = 0.00070181\n",
      "Iteration 153, loss = 0.00069525\n",
      "Iteration 154, loss = 0.00068738\n",
      "Iteration 155, loss = 0.00068173\n",
      "Iteration 156, loss = 0.00067870\n",
      "Iteration 157, loss = 0.00067540\n",
      "Iteration 158, loss = 0.00067052\n",
      "Iteration 159, loss = 0.00066248\n",
      "Iteration 160, loss = 0.00065375\n",
      "Iteration 161, loss = 0.00065174\n",
      "Iteration 162, loss = 0.00064424\n",
      "Iteration 163, loss = 0.00064216\n",
      "Iteration 164, loss = 0.00063362\n",
      "Iteration 165, loss = 0.00062846\n",
      "Iteration 166, loss = 0.00062158\n",
      "Iteration 167, loss = 0.00061983\n",
      "Iteration 168, loss = 0.00061511\n",
      "Iteration 169, loss = 0.00060812\n",
      "Iteration 170, loss = 0.00060427\n",
      "Iteration 171, loss = 0.00059642\n",
      "Iteration 172, loss = 0.00059150\n",
      "Iteration 173, loss = 0.00058892\n",
      "Iteration 174, loss = 0.00057948\n",
      "Iteration 175, loss = 0.00057681\n",
      "Iteration 176, loss = 0.00057240\n",
      "Iteration 177, loss = 0.00056825\n",
      "Iteration 178, loss = 0.00056468\n",
      "Iteration 179, loss = 0.00055873\n",
      "Iteration 180, loss = 0.00055607\n",
      "Iteration 181, loss = 0.00055417\n",
      "Iteration 182, loss = 0.00054777\n",
      "Iteration 183, loss = 0.00054797\n",
      "Iteration 184, loss = 0.00053981\n",
      "Iteration 185, loss = 0.00053674\n",
      "Iteration 186, loss = 0.00053067\n",
      "Iteration 187, loss = 0.00052682\n",
      "Iteration 188, loss = 0.00052303\n",
      "Iteration 189, loss = 0.00051890\n",
      "Iteration 190, loss = 0.00051551\n",
      "Iteration 191, loss = 0.00051167\n",
      "Iteration 192, loss = 0.00050843\n",
      "Iteration 193, loss = 0.00050416\n",
      "Iteration 194, loss = 0.00049968\n",
      "Iteration 195, loss = 0.00049803\n",
      "Iteration 196, loss = 0.00049476\n",
      "Iteration 197, loss = 0.00049248\n",
      "Iteration 198, loss = 0.00048906\n",
      "Iteration 199, loss = 0.00048684\n",
      "Iteration 200, loss = 0.00048264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# ANN(Training)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 0, shuffle =True)\n",
    "\n",
    "\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(150,100),# 은닉층 수 ex) (300,100) 유닛 10개짜리 은닉층 2개\n",
    "               #activation = 'logistic', #  적용함수 = 활성 함수(로지스틱 커브)\n",
    "               #activation = 'identity', # 학습속도가 엄청 빠름 결과 값 비교해보기\n",
    "               #activation = 'tanh', #결과값 제일 좋다\n",
    "               activation = 'relu', # 결과값 best\n",
    "               solver = 'sgd', # algorithm (sgd말고 2개 더있음 Gradient descent이해 하면 금방 이해한다.)\n",
    "               tol = 1e-7,  # 우리가 러닝을 했는대 더이상 낮아지지 않으면 공부 그만해 라는 뜻\n",
    "               learning_rate_init = .1,\n",
    "               verbose = True) # 보고 할 것인가 안할 것인가. False로 바꾸면 lossfuction값 안나옴\n",
    "\n",
    "\n",
    "model1 = mlp3.fit(x_train,y_train) # model 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(x_test)\n",
    "predictions1 = mlp1.predict(x_test)\n",
    "predictions2 = mlp2.predict(x_test)\n",
    "predictions3 = mlp3.predict(x_test)\n",
    "\n",
    "\n",
    "for i in range(0,len(x_test)):\n",
    "    if i % 20 == 0:\n",
    "        print() \n",
    "    \n",
    "    #print(y_test[i],predictions[i], end = \" ,\")\n",
    "# print(y_test)\n",
    "# print(predictions[1])    \n",
    "# print(len(x_test))\n",
    "# print(len(y_test))\n",
    "# print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>25</td>\n",
       "      <td>69</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1  All\n",
       "True                  \n",
       "0          23   0   23\n",
       "1           2  69   71\n",
       "All        25  69   94"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# panda로 만든 confusion matrix \n",
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(y_test,predictions, rownames = ['True'],colnames = ['Predicted'],margins=True)\n",
    "#pd.crosstab(y_test,predictions1, rownames = ['True'],colnames = ['Predicted'],margins=True)\n",
    "#pd.crosstab(y_test,predictions2, rownames = ['True'],colnames = ['Predicted'],margins=True)\n",
    "#pd.crosstab(y_test,predictions3, rownames = ['True'],colnames = ['Predicted'],margins=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"정확도 %.2f\" %accuracy_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n",
      "[0.         0.97183099 1.        ]\n",
      "[2 1 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-50c3a7924b81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FPR'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TPR'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mplot_roc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-50c3a7924b81>\u001b[0m in \u001b[0;36mplot_roc_curve\u001b[1;34m(fpr, tpr, label)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_roc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'k--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# ROC-curve 그리기 (1차 )\n",
    "from sklearn.metrics import roc_curve\n",
    "from \n",
    "fpr,tpr,thresholds = roc_curve(y_test,predictions)\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n",
    "def plot_roc_curve(fpr,tpr,label =None):\n",
    "    plt.plot(fpr,tpr,linewidth = 2, label = label)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "plot_roc_curve(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
